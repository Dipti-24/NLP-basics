# -*- coding: utf-8 -*-
"""RE_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ulpHm_X3UuRnuDTee105I3Bh5OfveaTq
"""

import re

"""Preprocessing"""

#remove punctuation
text="hello, world! this is a sample,"
cleaned_text=re.sub(r'[^\w\s]','',text)
print(cleaned_text)

#REMOVE URLS
text="website :https://www.indiabix.com/aptitude/profit-and-loss/"
c=re.sub(r'https\S+|www\S+|\S+.com\S+','',text)
c

#REMOVE STOP WORDS
import nltk
nltk.download("stopwords")
from nltk.corpus import stopwords
stopwords = set(stopwords.words("english"))
text="A vendor bought toffees at 6 for a rupee. How many for a rupee must he sell to gain 20%?"
c=" ".join(i for i in text.split() if i.lower() not in stopwords )
c

#match email

def mat_email(email):
  pattern= r'^[\w\.-]+0[\w\.-]+\.\w+$'
  if re.match(pattern,email):
    print("valid")
  else:
    print("not valid")
email1="examle@email.com"
email2="skjskhh"
mat_email(email1)
mat_email(email2)

#extracting phone numbers
def phn(text):
  pattern=r'\b\d{3}-\d{3}-\d{4}\b'
  phn_no=re.findall(pattern,text)
  print("phone no:")
  for i in phn_no:
    print(i)
text="contact me at 123-340-3335"
phn(text)

#tokenization
import nltk
nltk.download("punkt")
paragraph="A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs."
count=1
for i in nltk.sent_tokenize(paragraph):
  count=count+1
  print(i,count,"\n")

et=1
for i in nltk.word_tokenize(paragraph):
  print(et,i)
  et+=1

#pos tag
nltk.download('averaged_perceptron_tagger')
par="A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs."
tokens=nltk.word_tokenize(par)

pos_tags=nltk.pos_tag(tokens)

for i, tag in pos_tags:
  print(i,"             ",tag)

#Stemming
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
para="A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs."
sent=nltk.sent_tokenize(paragraph)
stemmer=PorterStemmer()

for i in range(len(sent)):
  words=nltk.word_tokenize(sent[i])
  words=[stemmer.stem(word) for word in words]
  sent[i] = ' '.join(words)
  print(sent[i])

#lemmatization
from nltk.stem import WordNetLemmatizer
nltk.download('wordnet')
from nltk.corpus import stopwords

para = "A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs."
sents = nltk.sent_tokenize(para)
lemmatizer = WordNetLemmatizer()

for i in range(len(sents)):
  #print(sent[i])
  words=nltk.word_tokenize(sents[i])
  #print(words)
  words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]
  sents[i]=' '.join(words)
  print(sents[i])

"""Feature Selection"""

import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import CountVectorizer

nltk.download('punkt')
nltk.download('stopwords')

para = "A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs."

sents = nltk.sent_tokenize(para)
corpus = []
ps = PorterStemmer()
stop_words = set(stopwords.words('english'))

for i in range(len(sents)):
    review = re.sub('[^a-zA-Z ]', ' ', sents[i])  # Keep only alphabetic characters and spaces
    review = review.lower()
    review = review.split()
    review = [ps.stem(word) for word in review if word not in stop_words]
    review = " ".join(review)
    corpus.append(review)

print(corpus)

cv = CountVectorizer(max_features=5)
x = cv.fit_transform(corpus).toarray()
print(x)

#creating tf-idf
from sklearn.feature_extraction.text import TfidfVectorizer
cv=TfidfVectorizer()
x=cv.fit_transform(corpus).toarray()
print(x)

#word2vect

!pip install gensim

import nltk
from gensim.models import Word2Vec
from nltk.corpus import stopwords
nltk.download('stopwords')

import re

para="A paragraph is a series of sentences that are organized and coherent, and are all related to a single topic. Almost every piece of writing you do that is longer than a few sentences should be organized into paragraphs."

text = re.sub(r'\[[0-9]*\]',' ',para)
text = re.sub(r'\s+',' ',text)
text = text.lower()
text = re.sub(r'\d',' ',text)
text = re.sub(r'\s+',' ',text)

#preparing daatset
senten=nltk.sent_tokenize(text)

senten=[nltk.word_tokenize(sentence) for sentence in senten]

for i in range(len(senten)):
  senten[i]=[word for word in senten[i] if word not in stopwords.words('english')]
print(senten)

#trining the word2vec

model = Word2Vec(senten,min_count=1)

print(model)

#finding word vector

vector = model.wv['series']
print(vector)


#most simliar words
print("%%%%%%5%55555")

for i in model.wv.most_similar('organized'):
  print(i)

import gensim
from gensim import corpora
from gensim.models import LdaModel
from gensim.models.coherencemodel import CoherenceModel

#sample doc

docs = [
    "the sky is blue",
    "the sun is bright",
    "the sun in the sky is bright",
    "the sky is cloudy"
]

#preproces the doc
tokenized_docs = [doc.lower().split() for doc in docs]

#create dic
dicts = corpora.Dictionary(tokenized_docs)

#create bag of words

corpus = [dicts.doc2bow(doc) for doc in tokenized_docs]

#build the lda model

lda_model = LdaModel(corpus=corpus, id2word=dicts, num_topics=2, random_state=42)

print("Topics: ")
for topic_id, topic in lda_model.show_topics(formatted=True):
  print(f"  Topic #{topic_id + 1 }: {topic}")

# associated keywords